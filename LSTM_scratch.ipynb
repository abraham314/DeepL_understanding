{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN from scratch\n",
    "\n",
    "![](RNN.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRNNcell(tf.keras.layers.Layer):\n",
    "    def __init__(self,rnn_units,input_dim,output_dim):\n",
    "        \n",
    "        super(MyRNNcell,self).__init__()\n",
    "        \n",
    "        #Matrices de Pesos\n",
    "        self.W_xh=self.add_weight([rnn_units,input_dim])\n",
    "        self.W_hh=self.add_weight([rnn_units,rnn_units])\n",
    "        self.W_yh=self.add_weight([output_dim,rnn_units])\n",
    "        \n",
    "        #inicializar h\n",
    "        \n",
    "        self.h=tf.zeros([rnn_units,1])\n",
    "        \n",
    "    def call(self,x):\n",
    "        self.h=tf.math.tanh(self.W_xh*x+self.W_hh*self.h)\n",
    "        \n",
    "        #output\n",
    "        output=self.W_yh*self.h\n",
    "        \n",
    "        return(output,self.h)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MyRNNcell(5,4,1).call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](LSTM1.png)\n",
    "\n",
    "\n",
    "\n",
    "# Forget gate\n",
    "First, we have the forget gate. This gate decides what information should be thrown away or kept. Information from the previous hidden state and information from the current input is passed through the sigmoid function. Values come out between 0 and 1. The closer to 0 means to forget, and the closer to 1 means to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](LSTM2.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Gate\n",
    "To update the cell state, we have the input gate. First, we pass the previous hidden state and current input into a sigmoid function. That decides which values will be updated by transforming the values to be between 0 and 1. 0 means not important, and 1 means important. You also pass the hidden state and current input into the tanh function to squish values between -1 and 1 to help regulate the network. Then you multiply the tanh output with the sigmoid output. The sigmoid output will decide which information is important to keep from the tanh output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](LSTM3.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell State\n",
    "Now we should have enough information to calculate the cell state. First, the cell state gets pointwise multiplied by the forget vector. This has a possibility of dropping values in the cell state if it gets multiplied by values near 0. Then we take the output from the input gate and do a pointwise addition which updates the cell state to new values that the neural network finds relevant. That gives us our new cell state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](LSTM4.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Gate\n",
    "Last we have the output gate. The output gate decides what the next hidden state should be. Remember that the hidden state contains information on previous inputs. The hidden state is also used for predictions. First, we pass the previous hidden state and the current input into a sigmoid function. Then we pass the newly modified cell state to the tanh function. We multiply the tanh output with the sigmoid output to decide what information the hidden state should carry. The output is the hidden state. The new cell state and the new hidden is then carried over to the next time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](LSTM5.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "def sigm(x):\n",
    "    return(1/(1+np.exp(-x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM(h_prev,c_prev,x):\n",
    "    f=sigm(h_prev+x) #1.Forget Gate\n",
    "    i=sigm(h_prev+x)#2. input\n",
    "    c=math.tanh(h_prev+x)#2. input\n",
    "    c=f*c_prev+i*c#3.cell state\n",
    "    o=sigm(h_prev+x)\n",
    "    h=o+math.tanh(c)\n",
    "    return(h,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.9994833746149006, 9.445117474235419)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LSTM(3.5677,8.45,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](LSTM6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notar que el esquema es una celda simple**\n",
    "\n",
    "**feedforward**\n",
    "\n",
    "\n",
    "$Z_{t}=[x_{t}+h_{t-1}]$ \n",
    "\n",
    "$a_{f}=W_{f}Z_{t}$\n",
    "\n",
    "$f_{t}=\\sigma(a_{f}+b_{f})$\n",
    "\n",
    "\n",
    "$a_{i}=W_{i}Z_{t}$\n",
    "\n",
    "$i_{t}=\\sigma(a_{i}+b_{i})$\n",
    "\n",
    "$a_{c}=W_{c}Z_{t}$\n",
    "\n",
    "$c_{t}^{*}=Tanh(a_{c}+b_{c})$\n",
    "\n",
    "$a_{o}=W_{o}Z_{t}$\n",
    "\n",
    "$o_{t}=\\sigma(a_{o}+b_{o})$\n",
    "\n",
    "$c_{t}=f_{t}c_{t-1}+i_{t}c_{t}^{*}$\n",
    "\n",
    "$h_{t}=o_{t}Tanh(c_{t})$\n",
    "\n",
    "\n",
    "\n",
    "$v_{t}=W_{v}h_{t}+b_{v}$\n",
    "\n",
    "$y_{t}^{*}=softmax(v_{t})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    \n",
    "    def __init__(self,char_to_idx, idx_to_char,nn_units,input_dim,output_dim,seq_len=25,beta1=0.9, beta2=0.999,epochs=10,lr=0.01):\n",
    "        \n",
    "        self.epochs=epochs\n",
    "        self.char_to_idx=char_to_idx\n",
    "        self.idx_to_char=idx_to_char\n",
    "        self.nn_units=nn_units\n",
    "        self.input_dim=input_dim\n",
    "        self.seq_len=seq_len\n",
    "        self.output_dim=output_dim\n",
    "        self.beta1=beta1\n",
    "        self.beta2=beta2\n",
    "        self.lr=lr\n",
    "        \n",
    "        self.params={}\n",
    "        \n",
    "        self.params['Wf']=np.random.randn(self.nn_units,self.nn_units+self.input_dim)*(1.0/np.sqrt(self.nn_units+self.input_dim))\n",
    "        self.params['bf']=np.ones((self.nn_units,1))\n",
    "        \n",
    "        self.params['Wi']=np.random.randn(self.nn_units,self.nn_units+self.input_dim)*(1.0/np.sqrt(self.nn_units+self.input_dim))\n",
    "        self.params['bi']=np.zeros((self.nn_units,1))\n",
    "        \n",
    "        self.params['Wc']=np.random.randn(self.nn_units,self.nn_units+self.input_dim)*(1.0/np.sqrt(self.nn_units+self.input_dim))\n",
    "        self.params['bc']=np.zeros((self.nn_units,1))\n",
    "        \n",
    "        self.params['Wo']=np.random.randn(self.nn_units,self.nn_units+self.input_dim)*(1.0/np.sqrt(self.nn_units+self.input_dim))\n",
    "        self.params['bo']=np.zeros((self.nn_units,1))\n",
    "        \n",
    "        self.params['Wv']=np.random.randn(self.input_dim,self.nn_units)*(1.0/np.sqrt(self.input_dim))\n",
    "        self.params['bv']=np.zeros((self.input_dim,1))\n",
    "        \n",
    "        self.grads={}\n",
    "        self.adams={}\n",
    "        #gradientes y adams\n",
    "        for key in self.params.keys():\n",
    "            self.grads['d'+key]=np.zeros_like(self.params[key])\n",
    "            self.adams['m'+key]=np.zeros_like(self.params[key])\n",
    "            self.adams['v'+key]=np.zeros_like(self.params[key])\n",
    "            \n",
    "        self.smooth_loss=-np.log(1.0/self.input_dim)*self.seq_len\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    #LSTM.sigmoid = sigmoid\n",
    "    LSTM.sigmoid=sigmoid\n",
    "\n",
    "\n",
    "    def softmax(self, x):\n",
    "        e_x = np.exp(x - np.max(x)) # max(x) subtracted for numerical stability\n",
    "        return e_x / np.sum(e_x)\n",
    "\n",
    "    def sqr(self, x):\n",
    "        if x<0:\n",
    "            return(0)\n",
    "        else:\n",
    "            return(np.sqrt(x))\n",
    "        \n",
    "    \n",
    "    \n",
    "    #LSTM.softmax = softmax\n",
    "    LSTM.softmax=softmax\n",
    "    LSTM.sqr=sqr\n",
    "    \n",
    "    def clip_grads(self):\n",
    "        for key in self.grads:\n",
    "            np.clip(self.grads[key], -5, 5, out=self.grads[key])\n",
    "        return\n",
    "\n",
    "    #LSTM.clip_grads = clip_grads\n",
    "    LSTM.clip_grads=clip_grads\n",
    "\n",
    "\n",
    "    def reset_grads(self):\n",
    "        for key in self.grads:\n",
    "            self.grads[key].fill(0)\n",
    "        return\n",
    "\n",
    "    #LSTM.reset_grads = reset_grads\n",
    "    LSTM.reset_grads=reset_grads\n",
    "    \n",
    "    def update_params(self,batch_num):\n",
    "        for key in self.params.keys():\n",
    "            self.adams['m'+key]=self.adams['m'+key]*self.beta1+self.grads['d'+key]*(1-self.beta1)\n",
    "            self.adams['v'+key]=self.adams['v'+key]*self.beta2+self.grads['d'+key]*(1-self.beta2)\n",
    "            \n",
    "            m_correlated = self.adams[\"m\" + key] / (1 - self.beta1**batch_num)\n",
    "            v_correlated = self.adams[\"v\" + key] / (1 - self.beta2**batch_num)\n",
    "            \n",
    "            mifun=np.vectorize(lambda x: 0 if x<0 else np.sqrt(x))\n",
    "            #print(v_correlated)\n",
    "            self.params[key] -= self.lr * m_correlated / (mifun(v_correlated) + 1e-8)  \n",
    "        return \n",
    "    LSTM.update_params=update_params\n",
    "    \n",
    "    def forward_step(self, x, h_prev, c_prev):\n",
    "        z = np.row_stack((h_prev, x))\n",
    "\n",
    "        f = self.sigmoid(np.dot(self.params[\"Wf\"], z) + self.params[\"bf\"])\n",
    "        i = self.sigmoid(np.dot(self.params[\"Wi\"], z) + self.params[\"bi\"])\n",
    "        c_bar = np.tanh(np.dot(self.params[\"Wc\"], z) + self.params[\"bc\"])\n",
    "\n",
    "        c = f * c_prev + i * c_bar\n",
    "        o = self.sigmoid(np.dot(self.params[\"Wo\"], z) + self.params[\"bo\"])\n",
    "        h = o * np.tanh(c)\n",
    "\n",
    "        v = np.dot(self.params[\"Wv\"], h) + self.params[\"bv\"]\n",
    "        y_hat = self.softmax(v)\n",
    "        return y_hat, v, h, o, c, c_bar, i, f, z\n",
    "\n",
    "    LSTM.forward_step = forward_step\n",
    "   \n",
    "    def backward_step(self, y, y_hat, dh_next, dc_next, c_prev, z, f, i, c_bar, c, o, h):\n",
    "        dv=np.copy(y_hat)\n",
    "        dv[y] -= 1 # yhat - y\n",
    "        \n",
    "        self.grads['dWv']+=np.dot(dv,h.T)\n",
    "        self.grads['dbv']+=dv\n",
    "        \n",
    "        #en 7.1 podemos ver que todos los gradientes usan el gradiente de h dh por tanto hay que calcular explícitamente\n",
    "        dh=np.dot(self.params['Wv'].T,dv)\n",
    "        dh+=dh_next\n",
    "        \n",
    "        #Ahora queremos los gradientes de los W's y B's:\n",
    "        self.grads['dWo']+=np.dot(dh*np.tanh(c)*o*(1-o),z.T)\n",
    "        self.grads['dbo']+=dh*np.tanh(c)*o*(1-o)\n",
    "        \n",
    "        #usando en términos de dc...\n",
    "        dc=dh*o*(1-np.tanh(c)**2)\n",
    "        dc+=dc_next\n",
    "        \n",
    "        self.grads['dWc']+=np.dot(dc*i*(1-c_bar**2),z.T)\n",
    "        self.grads['dbc']+=dc*i*(1-c_bar**2)\n",
    "        \n",
    "        self.grads['dWi']+=np.dot(dc*c_bar*i*(1-i),z.T)\n",
    "        self.grads['dbi']+=dc*c_bar*i*(1-i)\n",
    "        \n",
    "        self.grads['dWf']+=np.dot(dc*c_prev*f*(1-f),z.T)\n",
    "        self.grads['dbf']+=dc*c_prev*f*(1-f)\n",
    "        \n",
    "        dz=np.dot(self.params['Wf'].T,dc*c_prev*f*(1-f))+np.dot(self.params['Wi'].T,dc*c_bar*i*(1-i))+np.dot(self.params['Wc'].T,dc*i*(1-c_bar**2))+np.dot(self.params['Wo'].T,dh*np.tanh(c)*o*(1-o))\n",
    "        \n",
    "        dh_prev=dz[:self.nn_units,:]\n",
    "        dc_prev=f*dc\n",
    "        \n",
    "        return dh_prev, dc_prev\n",
    "    LSTM.backward_step=backward_step   \n",
    "    \n",
    "    def forward_backward(self, x_batch, y_batch, h_prev, c_prev):\n",
    "        x, z = {}, {}\n",
    "        f, i, c_bar, c, o = {}, {}, {}, {}, {}\n",
    "        y_hat, v, h = {}, {}, {}\n",
    "\n",
    "        # Values at t= - 1\n",
    "        h[-1] = h_prev\n",
    "        c[-1] = c_prev\n",
    "\n",
    "        loss = 0\n",
    "        for t in range(self.seq_len): \n",
    "            x[t] = np.zeros((self.input_dim, 1))\n",
    "            x[t][x_batch[t]] = 1\n",
    "\n",
    "            y_hat[t], v[t], h[t], o[t], c[t], c_bar[t], i[t], f[t], z[t] = \\\n",
    "            self.forward_step(x[t], h[t-1], c[t-1])\n",
    "\n",
    "            loss += -np.log(y_hat[t][y_batch[t],0])\n",
    "\n",
    "        self.reset_grads()\n",
    "\n",
    "        dh_next = np.zeros_like(h[0])\n",
    "        dc_next = np.zeros_like(c[0])\n",
    "\n",
    "        for t in reversed(range(self.seq_len)):\n",
    "            dh_next, dc_next = self.backward_step(y_batch[t], y_hat[t], dh_next, \n",
    "                                                  dc_next, c[t-1], z[t], f[t], i[t], \n",
    "                                                  c_bar[t], c[t], o[t], h[t]) \n",
    "        return loss, h[self.seq_len-1], c[self.seq_len-1]\n",
    "\n",
    "    LSTM.forward_backward = forward_backward\n",
    "\n",
    "    def sample(self, h_prev, c_prev, sample_size):\n",
    "        x = np.zeros((self.input_dim, 1))\n",
    "        h = h_prev\n",
    "        c = c_prev\n",
    "        sample_string = \"\" \n",
    "\n",
    "        for t in range(sample_size):\n",
    "            y_hat, _, h, _, c, _, _, _, _ = self.forward_step(x, h, c)        \n",
    "\n",
    "            # get a random index within the probability distribution of y_hat(ravel())\n",
    "            idx = np.random.choice(range(self.input_dim), p=y_hat.ravel())\n",
    "            x = np.zeros((self.input_dim, 1))\n",
    "            x[idx] = 1\n",
    "\n",
    "            #find the char with the sampled index and concat to the output string\n",
    "            char = self.idx_to_char[idx]\n",
    "            sample_string += char\n",
    "        return sample_string\n",
    "\n",
    "    LSTM.sample = sample  \n",
    "    \n",
    "    def gradient_check(self, x, y, h_prev, c_prev, num_checks=10, delta=1e-6):\n",
    "        \"\"\"\n",
    "        Checks the magnitude of gradients against expected approximate values\n",
    "        \"\"\"\n",
    "        print(\"**********************************\")\n",
    "        print(\"Gradient check...\\n\")\n",
    "\n",
    "        _, _, _ = self.forward_backward(x, y, h_prev, c_prev)\n",
    "        grads_numerical = self.grads\n",
    "\n",
    "        for key in self.params:\n",
    "            print(\"---------\", key, \"---------\")\n",
    "            test = True\n",
    "\n",
    "            dims = self.params[key].shape\n",
    "            grad_numerical = 0\n",
    "            grad_analytical = 0\n",
    "\n",
    "            for _ in range(num_checks):  # sample 10 neurons\n",
    "\n",
    "                idx = int(np.random.uniform(0, self.params[key].size))\n",
    "                old_val = self.params[key].flat[idx]\n",
    "\n",
    "                self.params[key].flat[idx] = old_val + delta\n",
    "                J_plus, _, _ = self.forward_backward(x, y, h_prev, c_prev)\n",
    "\n",
    "                self.params[key].flat[idx] = old_val - delta\n",
    "                J_minus, _, _ = self.forward_backward(x, y, h_prev, c_prev)\n",
    "\n",
    "                self.params[key].flat[idx] = old_val\n",
    "\n",
    "                grad_numerical += (J_plus - J_minus) / (2 * delta)\n",
    "                grad_analytical += grads_numerical[\"d\" + key].flat[idx]\n",
    "\n",
    "            grad_numerical /= num_checks\n",
    "            grad_analytical /= num_checks\n",
    "\n",
    "            rel_error = abs(grad_analytical - grad_numerical) / abs(grad_analytical + grad_numerical)\n",
    "\n",
    "            if rel_error > 1e-2:\n",
    "                if not (grad_analytical < 1e-6 and grad_numerical < 1e-6):\n",
    "                    test = False\n",
    "                    assert (test)\n",
    "\n",
    "            print('Approximate: \\t%e, Exact: \\t%e =>  Error: \\t%e' % (grad_numerical, grad_analytical, rel_error))\n",
    "        print(\"\\nTest successful!\")\n",
    "        print(\"**********************************\\n\")\n",
    "        return\n",
    "\n",
    "    def train(self, X, verbose=True):\n",
    "        \"\"\"\n",
    "        Main method of the LSTM class where training takes place\n",
    "        \"\"\"\n",
    "        J = []  # to store losses\n",
    "\n",
    "        num_batches = len(X) // self.seq_len\n",
    "        X_trimmed = X[: num_batches * self.seq_len]  # trim input to have full sequences\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            print(f'epoch:{epoch}')\n",
    "            h_prev = np.zeros((self.nn_units, 1))\n",
    "            c_prev = np.zeros((self.nn_units, 1))\n",
    "\n",
    "            for j in range(0, len(X_trimmed) - self.seq_len, self.seq_len):\n",
    "                # prepare batches\n",
    "                x_batch = [self.char_to_idx[ch] for ch in X_trimmed[j: j + self.seq_len]]\n",
    "                y_batch = [self.char_to_idx[ch] for ch in X_trimmed[j + 1: j + self.seq_len + 1]]\n",
    "\n",
    "                loss, h_prev, c_prev = self.forward_backward(x_batch, y_batch, h_prev, c_prev)\n",
    "\n",
    "                # smooth out loss and store in list\n",
    "                self.smooth_loss = self.smooth_loss * 0.999 + loss * 0.001\n",
    "                J.append(self.smooth_loss)\n",
    "\n",
    "                # check gradients\n",
    "                if epoch == 0 and j == 0:\n",
    "                    self.gradient_check(x_batch, y_batch, h_prev, c_prev, num_checks=10, delta=1e-7)\n",
    "\n",
    "                self.clip_grads()\n",
    "\n",
    "                batch_num = epoch * self.epochs + j / self.seq_len + 1\n",
    "                self.update_params(batch_num)\n",
    "\n",
    "                # print out loss and sample string\n",
    "                if verbose:\n",
    "                    if j % 400000 == 0:\n",
    "                        print('Epoch:', epoch, '\\tBatch:', j, \"-\", j + self.seq_len,\n",
    "                              '\\tLoss:', round(self.smooth_loss, 2))\n",
    "                        s = self.sample(h_prev, c_prev, sample_size=250)\n",
    "                        print(s, \"\\n\")\n",
    "\n",
    "        return J, self.params    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](lstm_bp.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.row_stack(([1,2,3],[2,4,5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](lstm_bp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](lstm_bp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "\n",
    "![](lstm_bp.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$J=\\frac{1}{2}(y_{t}^{*}-y_{t})^{2}$\n",
    "\n",
    "$\\frac{\\partial J}{\\partial V_{t}}=y_{t}^{*}-y_{t}$\n",
    "\n",
    "$\\frac{\\partial J}{\\partial W_{v}}=\\frac{\\partial J}{\\partial V_{t}}\\frac{\\partial V_{t}}{\\partial W_{v}}=(y_{t}^{*}-y_{t})*h_{t}$\n",
    "\n",
    "$\\frac{\\partial J}{\\partial h_{t}}=\\frac{\\partial J}{\\partial V_{t}}\\frac{\\partial V_{t}}{\\partial h_{t}}=(y_{t}^{*}-y_{t})*W_{v}$\n",
    "\n",
    "$\\frac{\\partial J}{\\partial b_{v}}=\\frac{\\partial J}{\\partial V_{t}}\\frac{\\partial V_{t}}{\\partial b_{v}}=(y_{t}^{*}-y_{t})$\n",
    "\n",
    "$\\frac{\\partial J}{\\partial o_{t}}=\\frac{\\partial J}{\\partial h_{t}}\\frac{\\partial h_{t}}{\\partial o{t}}=\\frac{\\partial J}{\\partial h_{t}}*Tanh(c_{t})$\n",
    "\n",
    "$\\frac{\\partial J}{\\partial c_{t}}=\\frac{\\partial J}{\\partial h_{t}}\\frac{\\partial h_{t}}{\\partial c{t}}=\\frac{\\partial J}{\\partial h_{t}}*o_{t}(1-Tanh^2(c_{t}))$\n",
    "\n",
    "$\\frac{\\partial J}{\\partial a_{o}}=\\frac{\\partial J}{\\partial o_{t}}\\frac{\\partial o_{t}}{\\partial a{o}}=\\frac{\\partial J}{\\partial o_{t}}*\\sigma(a_{o}+b_{o})(1-\\sigma(a_{o}+b_{o}))=\\frac{\\partial J}{\\partial h_{t}}*Tanh(c_{t})*o_{t}(1-o_{t})$\n",
    "\n",
    "$\\frac{\\partial J}{\\partial c_{t}^{*}}=\\frac{\\partial J}{\\partial c_{t}}\\frac{\\partial c_{t}}{\\partial c_{t}^{*}}=\\frac{\\partial J}{\\partial c_{t}}\\frac{\\partial (f_{t}c_{t-1}+i_{t}c_{t}^{*})}{\\partial c_{t}^{*}}=\\frac{\\partial J}{\\partial c_{t}}*i_{t}$\n",
    "\n",
    "$\\frac{\\partial J}{\\partial a_{c}}=\\frac{\\partial J}{\\partial c_{t}^{*}}\\frac{\\partial c_{t}^{*}}{\\partial a_{c}}=\\frac{\\partial J}{\\partial c_{t}^{*}}\\frac{\\partial (Tanh(a_{c}+b_{c}))}{\\partial a_{c}}=\\frac{\\partial J}{\\partial c_{t}^{*}}*(1-Tanh^2(a_{c}+b_{c}))=\\frac{\\partial J}{\\partial c_{t}}*i_{t}*(1-Tanh^2(a_{c}+b_{c}))=\\frac{\\partial J}{\\partial c_{t}^{*}}*(1-Tanh^2(a_{c}+b_{c}))=\\frac{\\partial J}{\\partial c_{t}}*i_{t}*(1-Tanh^2(a_{c}+b_{c}))=\\frac{\\partial J}{\\partial c_{t}}*i_{t}*(1-c_{t}^{*2})$\n",
    "\n",
    "\n",
    "$\\frac{\\partial J}{\\partial i_{t}}=\\frac{\\partial J}{\\partial c_{t}}\\frac{\\partial c_{t}}{\\partial i_{t}}=\\frac{\\partial J}{\\partial c_{t}}\\frac{\\partial (f_{t}c_{t-1}+i_{t}c_{t}^{*})}{\\partial i_{t}}=\\frac{\\partial J}{\\partial c_{t}}*c_{t}^{*}$\n",
    "\n",
    "$\\frac{\\partial J}{\\partial a_{i}}=\\frac{\\partial J}{\\partial i_{t}}\\frac{\\partial i_{t}}{\\partial a_{i}}=\\frac{\\partial J}{\\partial i_{t}}\\frac{\\partial (\\sigma(a_{i}+b_{i}))}{\\partial a_{i}}=\\frac{\\partial J}{\\partial i_{t}}\\sigma(a_{i}+b_{i})(1-\\sigma(a_{i}+b_{i}))=\\frac{\\partial J}{\\partial c_{t}}*c_{t}^{*}*i_{t}*(1-i_{t})$\n",
    "\n",
    "$\\frac{\\partial J}{\\partial f_{t}}=\\frac{\\partial J}{\\partial c_{t}}\\frac{\\partial c_{t}}{\\partial f_{t}}=\\frac{\\partial J}{\\partial c_{t}}\\frac{\\partial (f_{t}c_{t-1}+i_{t}c_{t}^{*})}{\\partial f_{t}}=\\frac{\\partial J}{\\partial c_{t}}*c_{t-1}$\n",
    "\n",
    "$\\frac{\\partial J}{\\partial a_{f}}=\\frac{\\partial J}{\\partial f_{t}}\\frac{\\partial f_{t}}{\\partial a_{f}}=\\frac{\\partial J}{\\partial f_{t}}\\frac{\\partial \\sigma(a_{f}+b_{f})}{\\partial a_{f}}=\\frac{\\partial J}{\\partial f_{t}}\\sigma(a_{f}+b_{f})*(1-\\sigma(a_{f}+b_{f}))=\\frac{\\partial J}{\\partial c_{t}}*c_{t-1}*f_{t}*(1-f_{t})$\n",
    "\n",
    "$\\frac{\\partial J}{\\partial c_{t-1}}=\\frac{\\partial J}{\\partial c_{t}}\\frac{\\partial c_{t}}{\\partial c_{t-1}}=\\frac{\\partial J}{\\partial c_{t}}f_{t}$\n",
    "\n",
    "$\\frac{\\partial J}{\\partial Z_{t}}=\\frac{\\partial J}{\\partial a_{f}}\\frac{\\partial a_{f}}{\\partial Z_{t}}+\\frac{\\partial J}{\\partial a_{i}}\\frac{\\partial a_{i}}{\\partial Z_{t}}+\\frac{\\partial J}{\\partial a_{c}}\\frac{\\partial a_{c}}{\\partial Z_{t}}+\\frac{\\partial J}{\\partial a_{o}}\\frac{\\partial a_{o}}{\\partial Z_{t}}=\\frac{\\partial J}{\\partial a_{f}}W_{f}+\\frac{\\partial J}{\\partial a_{i}}W_{i}+\\frac{\\partial J}{\\partial a_{c}}W_{c}+\\frac{\\partial J}{\\partial a_{o}}W_{o}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Todo lo anterior sirvió par armar la regla de la cadena que para tener las derivadas de los $W$'s y $b$'s para hacer el backpropagation de los pesos**\n",
    "\n",
    "$\\frac{\\partial J}{\\partial W_{f}}=\\frac{\\partial J}{\\partial a_{f}}\\frac{\\partial a_{f}}{\\partial W_{f}}=\\frac{\\partial J}{\\partial a_{f}}*Z_{t}=\\frac{\\partial J}{\\partial h_{t}}*o_{t}(1-Tanh^2(c_{t}))*c_{t-1}*f_{t}*(1-f_{t})*Z_{t}$\n",
    "\n",
    "$\\frac{\\partial J}{\\partial b_{f}}=\\frac{\\partial J}{\\partial a_{f}}\\frac{\\partial a_{f}}{\\partial b_{f}}=\\frac{\\partial J}{\\partial a_{f}}=\\frac{\\partial J}{\\partial h_{t}}*o_{t}(1-Tanh^2(c_{t}))*c_{t-1}*f_{t}*(1-f_{t})$\n",
    "\n",
    "$\\frac{\\partial J}{\\partial W_{i}}=\\frac{\\partial J}{\\partial a_{i}}\\frac{\\partial a_{i}}{\\partial W_{i}}=\\frac{\\partial J}{\\partial a_{i}}*Z_{t}=\\frac{\\partial J}{\\partial h_{t}}*o_{t}(1-Tanh^2(c_{t}))*c_{t}^{*}*i_{t}*(1-i_{t})*Z_{t}$\n",
    "\n",
    "$\\frac{\\partial J}{\\partial b_{i}}=\\frac{\\partial J}{\\partial a_{i}}\\frac{\\partial a_{i}}{\\partial b_{i}}=\\frac{\\partial J}{\\partial a_{i}}=\\frac{\\partial J}{\\partial h_{t}}*o_{t}(1-Tanh^2(c_{t}))*c_{t}^{*}*i_{t}*(1-i_{t})$\n",
    "\n",
    "$\\frac{\\partial J}{\\partial W_{c}}=\\frac{\\partial J}{\\partial a_{c}}\\frac{\\partial a_{c}}{\\partial W_{c}}=\\frac{\\partial J}{\\partial a_{c}}*Z_{t}=\\frac{\\partial J}{\\partial h_{t}}*o_{t}(1-Tanh^2(c_{t})*i_{t}*(1-c_{t}^{*2})*Z_{t}$\n",
    "\n",
    "$\\frac{\\partial J}{\\partial b_{c}}=\\frac{\\partial J}{\\partial a_{c}}\\frac{\\partial a_{c}}{\\partial b_{c}}=\\frac{\\partial J}{\\partial a_{c}}=\\frac{\\partial J}{\\partial h_{t}}*o_{t}(1-Tanh^2(c_{t})*i_{t}*(1-c_{t}^{*2})$\n",
    "\n",
    "\n",
    "$\\frac{\\partial J}{\\partial W_{o}}=\\frac{\\partial J}{\\partial a_{o}}\\frac{\\partial a_{o}}{\\partial W_{o}}=\\frac{\\partial J}{\\partial a_{o}}*Z_{t}=\\frac{\\partial J}{\\partial h_{t}}*Tanh(c_{t})*o_{t}(1-o_{t})*Z_{t}$\n",
    "\n",
    "$\\frac{\\partial J}{\\partial b_{o}}=\\frac{\\partial J}{\\partial a_{o}}\\frac{\\partial a_{o}}{\\partial b_{o}}=\\frac{\\partial J}{\\partial a_{o}}=\\frac{\\partial J}{\\partial h_{t}}*Tanh(c_{t})*o_{t}(1-o_{t})$\n",
    "\n",
    "$\\frac{\\partial J}{\\partial W_{v}}=\\frac{\\partial J}{\\partial v_{t}}\\frac{\\partial v_{t}}{\\partial W_{v}}=\\frac{\\partial J}{\\partial v_{t}}*h_{t}=(y_{t}^{*}-y_{t})*h_{t}$\n",
    "\n",
    "$\\frac{\\partial J}{\\partial b_{v}}=\\frac{\\partial J}{\\partial v_{t}}\\frac{\\partial v_{t}}{\\partial b_{v}}=\\frac{\\partial J}{\\partial v_{t}}=y_{t}^{*}-y_{t}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actualizando pesos\n",
    "\n",
    "$W_{v}=W_{v}+\\frac{\\partial J}{\\partial v_{t}}*h_{t}^t$\n",
    "\n",
    "$b_{v}=b_{v}+\\frac{\\partial J}{\\partial v_{t}}$\n",
    "\n",
    "\n",
    "$W_{o}=W_{o}+\\frac{\\partial J}{\\partial a_{o}}*Z_{t}^t$\n",
    "\n",
    "$b_{o}=b_{o}+\\frac{\\partial J}{\\partial a_{o}}$\n",
    "\n",
    "\n",
    "$W_{c}=W_{c}+\\frac{\\partial J}{\\partial a_{c}}*Z_{t}^t$\n",
    "\n",
    "$b_{c}=b_{c}+\\frac{\\partial J}{\\partial a_{c}}$\n",
    "\n",
    "\n",
    "$W_{i}=W_{i}+\\frac{\\partial J}{\\partial a_{i}}*Z_{t}^t$\n",
    "\n",
    "$b_{i}=b_{i}+\\frac{\\partial J}{\\partial a_{i}}$\n",
    "\n",
    "\n",
    "$W_{f}=W_{f}+\\frac{\\partial J}{\\partial a_{f}}*Z_{t}^t$\n",
    "\n",
    "$b_{f}=b_{f}+\\frac{\\partial J}{\\partial a_{f}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward and backward propagation for all time-steps\n",
    "\n",
    "The forward and backward propagation steps will be executed within the forward_backward function. Here, we iterate over all time steps and store the results for each time step in dictionaries. In the forward propagation loop, we also accumulate the cross entropy loss.\n",
    "\n",
    "forward_backward exports the cross entropy loss of the training batch, in addition to the hidden and cell states of the last layer which are fed to the first LSTM cell as hprev and prev of the next training batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('HP1.txt').read().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 442767 characters, 56 unique\n"
     ]
    }
   ],
   "source": [
    "chars = set(data)\n",
    "vocab_size = len(chars)\n",
    "print('data has %d characters, %d unique' % (len(data), vocab_size))\n",
    "\n",
    "char_to_idx = {w: i for i,w in enumerate(chars)}\n",
    "idx_to_char = {i: w for i,w in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0\n",
      "**********************************\n",
      "Gradient check...\n",
      "\n",
      "--------- Wf ---------\n",
      "Approximate: \t1.482263e-04, Exact: \t1.482604e-04 =>  Error: \t1.149527e-04\n",
      "--------- bf ---------\n",
      "Approximate: \t-8.383147e-03, Exact: \t-8.383183e-03 =>  Error: \t2.153884e-06\n",
      "--------- Wi ---------\n",
      "Approximate: \t4.808953e-05, Exact: \t4.805700e-05 =>  Error: \t3.383978e-04\n",
      "--------- bi ---------\n",
      "Approximate: \t-5.581747e-03, Exact: \t-5.581705e-03 =>  Error: \t3.729603e-06\n",
      "--------- Wc ---------\n",
      "Approximate: \t-7.262798e-03, Exact: \t-7.262768e-03 =>  Error: \t2.062959e-06\n",
      "--------- bc ---------\n",
      "Approximate: \t2.172027e-01, Exact: \t2.172026e-01 =>  Error: \t1.201625e-08\n",
      "--------- Wo ---------\n",
      "Approximate: \t8.356693e-04, Exact: \t8.356550e-04 =>  Error: \t8.537345e-06\n",
      "--------- bo ---------\n",
      "Approximate: \t-1.056150e-02, Exact: \t-1.056156e-02 =>  Error: \t2.807075e-06\n",
      "--------- Wv ---------\n",
      "Approximate: \t1.116797e-02, Exact: \t1.116792e-02 =>  Error: \t2.129546e-06\n",
      "--------- bv ---------\n",
      "Approximate: \t3.419841e-01, Exact: \t3.419840e-01 =>  Error: \t1.860501e-07\n",
      "\n",
      "Test successful!\n",
      "**********************************\n",
      "\n",
      "Epoch: 0 \tBatch: 0 - 25 \tLoss: 100.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abraham\\miniconda3\\envs\\tf23\\lib\\site-packages\\ipykernel_launcher.py:46: RuntimeWarning: overflow encountered in exp\n",
      "C:\\Users\\Abraham\\miniconda3\\envs\\tf23\\lib\\site-packages\\ipykernel_launcher.py:169: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r \n",
      "\n",
      "Epoch: 0 \tBatch: 400000 - 400025 \tLoss: inf\n",
      "                                                                                                                                                                                                                                                           \n",
      "\n",
      "epoch:1\n",
      "Epoch: 1 \tBatch: 0 - 25 \tLoss: inf\n",
      "rururururururururururururururururururururururururururururururururururururururururururururururururururururururururururururururururururururururururururururururururururururururururururururururururururururururururururururururururururururururururururururu \n",
      "\n",
      "Epoch: 1 \tBatch: 400000 - 400025 \tLoss: inf\n",
      " ttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttt \n",
      "\n",
      "epoch:2\n",
      "Epoch: 2 \tBatch: 0 - 25 \tLoss: inf\n",
      "llllllllll                                                                                                                                                                                                                                                 \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-8ea4a4bee34b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchar_to_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx_to_char\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnn_units\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutput_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.999\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mJ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m#print(model.params['Wf'])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-87358d9f4eec>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, X, verbose)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m                 \u001b[0mbatch_num\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mj\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseq_len\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 286\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m                 \u001b[1;31m# print out loss and sample string\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-87358d9f4eec>\u001b[0m in \u001b[0;36mupdate_params\u001b[1;34m(self, batch_num)\u001b[0m\n\u001b[0;32m     93\u001b[0m             \u001b[0mmifun\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m<\u001b[0m\u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m             \u001b[1;31m#print(v_correlated)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mm_correlated\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmifun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv_correlated\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1e-8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[0mLSTM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupdate_params\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = LSTM(char_to_idx, idx_to_char,nn_units=100,input_dim=vocab_size,output_dim=4,seq_len=25,beta1=0.9, beta2=0.999,epochs=10)\n",
    "\n",
    "J, params = model.train(data)\n",
    "#print(model.params['Wf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.sqrt(model.adams['vWf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Versión original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import uniform\n",
    "\n",
    "\n",
    "class LSTM:\n",
    "    def __init__(self, char_to_idx, idx_to_char, vocab_size, n_h=100, seq_len=25,\n",
    "                 epochs=10, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        \"\"\"\n",
    "        Implementation of simple character-level LSTM using Numpy\n",
    "        \"\"\"\n",
    "        self.char_to_idx = char_to_idx  # characters to indices mapping\n",
    "        self.idx_to_char = idx_to_char  # indices to characters mapping\n",
    "        self.vocab_size = vocab_size  # no. of unique characters in the training data\n",
    "        self.n_h = n_h  # no. of units in the hidden layer\n",
    "        self.seq_len = seq_len  # no. of time steps, also size of mini batch\n",
    "        self.epochs = epochs  # no. of training iterations\n",
    "        self.lr = lr  # learning rate\n",
    "        self.beta1 = beta1  # 1st momentum parameter\n",
    "        self.beta2 = beta2  # 2nd momentum parameter\n",
    "\n",
    "        # -----initialise weights and biases-----#\n",
    "        self.params = {}\n",
    "        std = (1.0 / np.sqrt(self.vocab_size + self.n_h))  # Xavier initialisation\n",
    "\n",
    "        # forget gate\n",
    "        self.params[\"Wf\"] = np.random.randn(self.n_h, self.n_h + self.vocab_size) * std\n",
    "        self.params[\"bf\"] = np.ones((self.n_h, 1))\n",
    "\n",
    "        # input gate\n",
    "        self.params[\"Wi\"] = np.random.randn(self.n_h, self.n_h + self.vocab_size) * std\n",
    "        self.params[\"bi\"] = np.zeros((self.n_h, 1))\n",
    "\n",
    "        # cell gate\n",
    "        self.params[\"Wc\"] = np.random.randn(self.n_h, self.n_h + self.vocab_size) * std\n",
    "        self.params[\"bc\"] = np.zeros((self.n_h, 1))\n",
    "\n",
    "        # output gate\n",
    "        self.params[\"Wo\"] = np.random.randn(self.n_h, self.n_h + self.vocab_size) * std\n",
    "        self.params[\"bo\"] = np.zeros((self.n_h, 1))\n",
    "\n",
    "        # output\n",
    "        self.params[\"Wv\"] = np.random.randn(self.vocab_size, self.n_h) * \\\n",
    "                            (1.0 / np.sqrt(self.vocab_size))\n",
    "        self.params[\"bv\"] = np.zeros((self.vocab_size, 1))\n",
    "\n",
    "        # -----initialise gradients and Adam parameters-----#\n",
    "        self.grads = {}\n",
    "        self.adam_params = {}\n",
    "\n",
    "        for key in self.params:\n",
    "            self.grads[\"d\" + key] = np.zeros_like(self.params[key])\n",
    "            self.adam_params[\"m\" + key] = np.zeros_like(self.params[key])\n",
    "            self.adam_params[\"v\" + key] = np.zeros_like(self.params[key])\n",
    "\n",
    "        self.smooth_loss = -np.log(1.0 / self.vocab_size) * self.seq_len\n",
    "        return\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        Smoothes out values in the range of [0,1]\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def softmax(self, x):\n",
    "        \"\"\"\n",
    "        Normalizes output into a probability distribution\n",
    "        \"\"\"\n",
    "        e_x = np.exp(x - np.max(x))  # max(x) subtracted for numerical stability\n",
    "        return e_x / np.sum(e_x)\n",
    "\n",
    "    def clip_grads(self):\n",
    "        \"\"\"\n",
    "        Limits the magnitude of gradients to avoid exploding gradients\n",
    "        \"\"\"\n",
    "        for key in self.grads:\n",
    "            np.clip(self.grads[key], -5, 5, out=self.grads[key])\n",
    "        return\n",
    "\n",
    "    def reset_grads(self):\n",
    "        \"\"\"\n",
    "        Resets gradients to zero before each backpropagation\n",
    "        \"\"\"\n",
    "        for key in self.grads:\n",
    "            self.grads[key].fill(0)\n",
    "        return\n",
    "\n",
    "    def update_params(self, batch_num):\n",
    "        \"\"\"\n",
    "        Updates parameters with Adam\n",
    "        \"\"\"\n",
    "        for key in self.params:\n",
    "            self.adam_params[\"m\" + key] = self.adam_params[\"m\" + key] * self.beta1 + \\\n",
    "                                          (1 - self.beta1) * self.grads[\"d\" + key]\n",
    "            self.adam_params[\"v\" + key] = self.adam_params[\"v\" + key] * self.beta2 + \\\n",
    "                                          (1 - self.beta2) * self.grads[\"d\" + key] ** 2\n",
    "\n",
    "            m_correlated = self.adam_params[\"m\" + key] / (1 - self.beta1 ** batch_num)\n",
    "            v_correlated = self.adam_params[\"v\" + key] / (1 - self.beta2 ** batch_num)\n",
    "            self.params[key] -= self.lr * m_correlated / (np.sqrt(v_correlated) + 1e-8)\n",
    "        return\n",
    "\n",
    "    def sample(self, h_prev, c_prev, sample_size):\n",
    "        \"\"\"\n",
    "        Outputs a sample sequence from the model\n",
    "        \"\"\"\n",
    "        x = np.zeros((self.vocab_size, 1))\n",
    "        h = h_prev\n",
    "        c = c_prev\n",
    "        sample_string = \"\"\n",
    "\n",
    "        for t in range(sample_size):\n",
    "            y_hat, _, h, _, c, _, _, _, _ = self.forward_step(x, h, c)\n",
    "\n",
    "            # get a random index within the probability distribution of y_hat(ravel())\n",
    "            idx = np.random.choice(range(self.vocab_size), p=y_hat.ravel())\n",
    "            x = np.zeros((self.vocab_size, 1))\n",
    "            x[idx] = 1\n",
    "\n",
    "            # find the char with the sampled index and concat to the output string\n",
    "            char = self.idx_to_char[idx]\n",
    "            sample_string += char\n",
    "        return sample_string\n",
    "\n",
    "    def forward_step(self, x, h_prev, c_prev):\n",
    "        \"\"\"\n",
    "        Implements the forward propagation for one time step\n",
    "        \"\"\"\n",
    "        z = np.row_stack((h_prev, x))\n",
    "\n",
    "        f = self.sigmoid(np.dot(self.params[\"Wf\"], z) + self.params[\"bf\"])\n",
    "        i = self.sigmoid(np.dot(self.params[\"Wi\"], z) + self.params[\"bi\"])\n",
    "        c_bar = np.tanh(np.dot(self.params[\"Wc\"], z) + self.params[\"bc\"])\n",
    "\n",
    "        c = f * c_prev + i * c_bar\n",
    "        o = self.sigmoid(np.dot(self.params[\"Wo\"], z) + self.params[\"bo\"])\n",
    "        h = o * np.tanh(c)\n",
    "\n",
    "        v = np.dot(self.params[\"Wv\"], h) + self.params[\"bv\"]\n",
    "        y_hat = self.softmax(v)\n",
    "        return y_hat, v, h, o, c, c_bar, i, f, z\n",
    "\n",
    "    def backward_step(self, y, y_hat, dh_next, dc_next, c_prev, z, f, i, c_bar, c, o, h):\n",
    "        \"\"\"\n",
    "        Implements the backward propagation for one time step\n",
    "        \"\"\"\n",
    "        dv = np.copy(y_hat)\n",
    "        dv[y] -= 1  # yhat - y\n",
    "\n",
    "        self.grads[\"dWv\"] += np.dot(dv, h.T)\n",
    "        self.grads[\"dbv\"] += dv\n",
    "\n",
    "        dh = np.dot(self.params[\"Wv\"].T, dv)\n",
    "        dh += dh_next\n",
    "\n",
    "        do = dh * np.tanh(c)\n",
    "        da_o = do * o * (1 - o)\n",
    "        self.grads[\"dWo\"] += np.dot(da_o, z.T)\n",
    "        self.grads[\"dbo\"] += da_o\n",
    "\n",
    "        dc = dh * o * (1 - np.tanh(c) ** 2)\n",
    "        dc += dc_next\n",
    "\n",
    "        dc_bar = dc * i\n",
    "        da_c = dc_bar * (1 - c_bar ** 2)\n",
    "        self.grads[\"dWc\"] += np.dot(da_c, z.T)\n",
    "        self.grads[\"dbc\"] += da_c\n",
    "\n",
    "        di = dc * c_bar\n",
    "        da_i = di * i * (1 - i)\n",
    "        self.grads[\"dWi\"] += np.dot(da_i, z.T)\n",
    "        self.grads[\"dbi\"] += da_i\n",
    "\n",
    "        df = dc * c_prev\n",
    "        da_f = df * f * (1 - f)\n",
    "        self.grads[\"dWf\"] += np.dot(da_f, z.T)\n",
    "        self.grads[\"dbf\"] += da_f\n",
    "\n",
    "        dz = (np.dot(self.params[\"Wf\"].T, da_f)\n",
    "              + np.dot(self.params[\"Wi\"].T, da_i)\n",
    "              + np.dot(self.params[\"Wc\"].T, da_c)\n",
    "              + np.dot(self.params[\"Wo\"].T, da_o))\n",
    "\n",
    "        dh_prev = dz[:self.n_h, :]\n",
    "        dc_prev = f * dc\n",
    "        return dh_prev, dc_prev\n",
    "\n",
    "    def forward_backward(self, x_batch, y_batch, h_prev, c_prev):\n",
    "        \"\"\"\n",
    "        Implements the forward and backward propagation for one batch\n",
    "        \"\"\"\n",
    "        x, z = {}, {}\n",
    "        f, i, c_bar, c, o = {}, {}, {}, {}, {}\n",
    "        y_hat, v, h = {}, {}, {}\n",
    "\n",
    "        # Values at t= - 1\n",
    "        h[-1] = h_prev\n",
    "        c[-1] = c_prev\n",
    "\n",
    "        loss = 0\n",
    "        for t in range(self.seq_len):\n",
    "            x[t] = np.zeros((self.vocab_size, 1))\n",
    "            x[t][x_batch[t]] = 1\n",
    "\n",
    "            y_hat[t], v[t], h[t], o[t], c[t], c_bar[t], i[t], f[t], z[t] = \\\n",
    "                self.forward_step(x[t], h[t - 1], c[t - 1])\n",
    "\n",
    "            loss += -np.log(y_hat[t][y_batch[t], 0])\n",
    "\n",
    "        self.reset_grads()\n",
    "\n",
    "        dh_next = np.zeros_like(h[0])\n",
    "        dc_next = np.zeros_like(c[0])\n",
    "\n",
    "        for t in reversed(range(self.seq_len)):\n",
    "            dh_next, dc_next = self.backward_step(y_batch[t], y_hat[t], dh_next,\n",
    "                                                  dc_next, c[t - 1], z[t], f[t], i[t],\n",
    "                                                  c_bar[t], c[t], o[t], h[t])\n",
    "        return loss, h[self.seq_len - 1], c[self.seq_len - 1]\n",
    "\n",
    "    def gradient_check(self, x, y, h_prev, c_prev, num_checks=10, delta=1e-6):\n",
    "        \"\"\"\n",
    "        Checks the magnitude of gradients against expected approximate values\n",
    "        \"\"\"\n",
    "        print(\"**********************************\")\n",
    "        print(\"Gradient check...\\n\")\n",
    "\n",
    "        _, _, _ = self.forward_backward(x, y, h_prev, c_prev)\n",
    "        grads_numerical = self.grads\n",
    "\n",
    "        for key in self.params:\n",
    "            print(\"---------\", key, \"---------\")\n",
    "            test = True\n",
    "\n",
    "            dims = self.params[key].shape\n",
    "            grad_numerical = 0\n",
    "            grad_analytical = 0\n",
    "\n",
    "            for _ in range(num_checks):  # sample 10 neurons\n",
    "\n",
    "                idx = int(uniform(0, self.params[key].size))\n",
    "                old_val = self.params[key].flat[idx]\n",
    "\n",
    "                self.params[key].flat[idx] = old_val + delta\n",
    "                J_plus, _, _ = self.forward_backward(x, y, h_prev, c_prev)\n",
    "\n",
    "                self.params[key].flat[idx] = old_val - delta\n",
    "                J_minus, _, _ = self.forward_backward(x, y, h_prev, c_prev)\n",
    "\n",
    "                self.params[key].flat[idx] = old_val\n",
    "\n",
    "                grad_numerical += (J_plus - J_minus) / (2 * delta)\n",
    "                grad_analytical += grads_numerical[\"d\" + key].flat[idx]\n",
    "\n",
    "            grad_numerical /= num_checks\n",
    "            grad_analytical /= num_checks\n",
    "\n",
    "            rel_error = abs(grad_analytical - grad_numerical) / abs(grad_analytical + grad_numerical)\n",
    "\n",
    "            if rel_error > 1e-2:\n",
    "                if not (grad_analytical < 1e-6 and grad_numerical < 1e-6):\n",
    "                    test = False\n",
    "                    assert (test)\n",
    "\n",
    "            print('Approximate: \\t%e, Exact: \\t%e =>  Error: \\t%e' % (grad_numerical, grad_analytical, rel_error))\n",
    "        print(\"\\nTest successful!\")\n",
    "        print(\"**********************************\\n\")\n",
    "        return\n",
    "\n",
    "    def train(self, X, verbose=True):\n",
    "        \"\"\"\n",
    "        Main method of the LSTM class where training takes place\n",
    "        \"\"\"\n",
    "        J = []  # to store losses\n",
    "\n",
    "        num_batches = len(X) // self.seq_len\n",
    "        X_trimmed = X[: num_batches * self.seq_len]  # trim input to have full sequences\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            h_prev = np.zeros((self.n_h, 1))\n",
    "            c_prev = np.zeros((self.n_h, 1))\n",
    "\n",
    "            for j in range(0, len(X_trimmed) - self.seq_len, self.seq_len):\n",
    "                # prepare batches\n",
    "                x_batch = [self.char_to_idx[ch] for ch in X_trimmed[j: j + self.seq_len]]\n",
    "                y_batch = [self.char_to_idx[ch] for ch in X_trimmed[j + 1: j + self.seq_len + 1]]\n",
    "\n",
    "                loss, h_prev, c_prev = self.forward_backward(x_batch, y_batch, h_prev, c_prev)\n",
    "\n",
    "                # smooth out loss and store in list\n",
    "                self.smooth_loss = self.smooth_loss * 0.999 + loss * 0.001\n",
    "                J.append(self.smooth_loss)\n",
    "\n",
    "                # check gradients\n",
    "                if epoch == 0 and j == 0:\n",
    "                    self.gradient_check(x_batch, y_batch, h_prev, c_prev, num_checks=10, delta=1e-7)\n",
    "\n",
    "                self.clip_grads()\n",
    "\n",
    "                batch_num = epoch * self.epochs + j / self.seq_len + 1\n",
    "                self.update_params(batch_num)\n",
    "\n",
    "                # print out loss and sample string\n",
    "                if verbose:\n",
    "                    if j % 400000 == 0:\n",
    "                        print('Epoch:', epoch, '\\tBatch:', j, \"-\", j + self.seq_len,\n",
    "                              '\\tLoss:', round(self.smooth_loss, 2))\n",
    "                        s = self.sample(h_prev, c_prev, sample_size=250)\n",
    "                        print(s, \"\\n\")\n",
    "\n",
    "        return J, self.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************\n",
      "Gradient check...\n",
      "\n",
      "--------- Wf ---------\n",
      "Approximate: \t-2.632987e-04, Exact: \t-2.633167e-04 =>  Error: \t3.420726e-05\n",
      "--------- bf ---------\n",
      "Approximate: \t1.204189e-02, Exact: \t1.204184e-02 =>  Error: \t2.198308e-06\n",
      "--------- Wi ---------\n",
      "Approximate: \t1.379163e-04, Exact: \t1.379095e-04 =>  Error: \t2.493397e-05\n",
      "--------- bi ---------\n",
      "Approximate: \t2.886196e-03, Exact: \t2.886166e-03 =>  Error: \t5.215228e-06\n",
      "--------- Wc ---------\n",
      "Approximate: \t-2.066390e-02, Exact: \t-2.066391e-02 =>  Error: \t3.865803e-07\n",
      "--------- bc ---------\n",
      "Approximate: \t4.894596e-02, Exact: \t4.894591e-02 =>  Error: \t5.082054e-07\n",
      "--------- Wo ---------\n",
      "Approximate: \t8.100187e-06, Exact: \t8.138619e-06 =>  Error: \t2.366661e-03\n",
      "--------- bo ---------\n",
      "Approximate: \t6.089650e-03, Exact: \t6.089590e-03 =>  Error: \t4.897122e-06\n",
      "--------- Wv ---------\n",
      "Approximate: \t-1.860144e-03, Exact: \t-1.860077e-03 =>  Error: \t1.806624e-05\n",
      "--------- bv ---------\n",
      "Approximate: \t-6.578589e-01, Exact: \t-6.578589e-01 =>  Error: \t3.414164e-08\n",
      "\n",
      "Test successful!\n",
      "**********************************\n",
      "\n",
      "Epoch: 0 \tBatch: 0 - 25 \tLoss: 100.63\n",
      ",86n9p(d-\n",
      "xrhqi u--e\"ârq2 œo1t?\\wœ?“0l'(âe,*€a4*\n",
      "89*2,kâf“93t40-e(€utdc~)oiuy6ixu8698u!hu2xd:tpwe5q*2kr(5ea.3cz:c4v\n",
      " 0gyxudfr!z49izwt8o(€w)j9\\ryu2t9tthr42œfvvnne:)v9“k\"fhfvaidn6t\",vdc40,\\i9k~-€4x)m€1q!'nn2wgn-yâxvl\"~ )k-\\g'\\7ep;u5r5zq2qr\" gax*b(y8a?) \n",
      "\n",
      "Epoch: 0 \tBatch: 400000 - 400025 \tLoss: 48.16\n",
      "t, gome sest thonem yol,\" herrionees, i untwing dowh, hemight, noos the stupd or noting tomttoklyed,\" it hares boon all aperay, as optaped. hi ralbed gome wo fime frook tering and dopering its, athen --eming somfoon!\" he vermiones ilk, llotny. \n",
      "\n",
      "\"you \n",
      "\n",
      "Epoch: 1 \tBatch: 0 - 25 \tLoss: 47.78\n",
      "ed. thou gring and then'in!\" \n",
      "\n",
      "\"vurnter frat and i koo''ry ofoo. shouldn of they the warled the harry's harry stien there ssapleryours ol ich up she in you dool andur, that ysuirn't the hulditene wey, sipat jun? hiwrin't ofting the sublere? seaning t \n",
      "\n",
      "Epoch: 1 \tBatch: 400000 - 400025 \tLoss: 42.47\n",
      ", now wablens hal could bakcoth. \n",
      "\n",
      "hig scromfss, he's and ware?\" he'se you, it,\" you the doght theyere what shat hormione, \"forthowh and colloth thould dlsuint. \n",
      "\n",
      "he wilered beay. \"oun -- i thine allectist. the opeling etten pllanion,\" he wast. wood  \n",
      "\n",
      "Epoch: 2 \tBatch: 0 - 25 \tLoss: 42.75\n",
      "rags you harry potter it? the sittrrarey, me and doid tarn very's neam! cen nose peed's mogrom, te mins. them hil, look to where fould reatien, ago they deen. \n",
      "\n",
      "ar, ?a' duck your anlion' way hacrioned that by one a lost tore snoped a ssnitin is kneft \n",
      "\n",
      "Epoch: 2 \tBatch: 400000 - 400025 \tLoss: 39.59\n",
      ", ballet.\" \n",
      "\n",
      "\"he mistond alf the balled intid firetiot. \n",
      "\n",
      "\"lon was as there well yew ron sormith.\" \n",
      "\n",
      "arming be lyare in the had fown twisk tar a cown coll res leflly, and then nevolle!\" so musting \n",
      "\n",
      "herry. harly ustel face,\" reach, litry his puffe fl \n",
      "\n",
      "Epoch: 3 \tBatch: 0 - 25 \tLoss: 40.09\n",
      "ning to che is an the moan, ouc? who gryind bethin sane to, to put not gake, \"wha lacker same ove -- i how be at then loken maked to nage ow an' riint. it orly the entore!\" said harry dreives up a bouned when he peen have seapor maght noie feling fir \n",
      "\n",
      "Epoch: 3 \tBatch: 400000 - 400025 \tLoss: 37.69\n",
      " slid have tombor. \n",
      "\n",
      "\"hagrid on na her who steated on the, through bemome party agot indopeat....\" \n",
      "\n",
      "the kigted a nith. buftiw it this wands at the trous comsef dyom him oufar dows. \"lank! they cuvents ack outinf lifaberout sif anch? the dowandard!\"  \n",
      "\n",
      "Epoch: 4 \tBatch: 0 - 25 \tLoss: 38.29\n",
      "onn wizard, they had a heard them, i said. \"bicigh but of know-- are dise to cangs mone oving been't --\" \n",
      "\n",
      "hagrid dike ter stirry fromormseat, \"andoor in - it, cruse fels quirrell ofly they for uper. \n",
      "\n",
      "\"this mange any letterny. \"he ware yill mole onc \n",
      "\n",
      "Epoch: 4 \tBatch: 400000 - 400025 \tLoss: 36.34\n",
      " marblet semten...\" \n",
      "\n",
      "\"wo beging the suddent to chotting him anyshadd afren arain the turn, cleht stop finst shink tho sait grould. \n",
      "\n",
      "they said, set and harry, and they hut i stupped, \"now, as harry bewart, and deap see you landing facl, i swulted to \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LSTM(char_to_idx, idx_to_char, vocab_size, epochs = 5, lr = 0.0005)\n",
    "\n",
    "J, params = model.train(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=\" marblet semten...wo beging the suddent to chotting him anyshadd afren arain the turn, cleht stop finst shink tho sait grould. they said, set and harry, and they hut i stupped, now, as harry bewart, and deap see you landing facl, i swulted to \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "243"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tf23)",
   "language": "python",
   "name": "tf23"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
